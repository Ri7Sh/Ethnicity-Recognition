{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten,Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds_path='dataset\\CFD 2.0.3 Images'\n",
    "# dataset = pd.read_excel(\"dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def class_val(i):\n",
    "    \n",
    "    if(i == 'A' or i =='F'):\n",
    "        p = 0\n",
    "    elif(i == 'B' or i =='M'):\n",
    "        p = 1\n",
    "    elif(i == 'L'):\n",
    "        p = 2\n",
    "    else:\n",
    "        p = 3\n",
    "    return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = os.listdir(ds_path)\n",
    "facedata = \"haarcascade_frontalface_default.xml\"\n",
    "cascade = cv2.CascadeClassifier(facedata)\n",
    "X = []\n",
    "Y = []\n",
    "Gender = []\n",
    "for file in ds:\n",
    "    \n",
    "    img_path = os.path.join(ds_path,file) \n",
    "    img_files = os.listdir(img_path)\n",
    "    for imfile in img_files:\n",
    "        if imfile.endswith(\".jpg\"):\n",
    "            imfile1 = os.path.join(img_path,imfile)\n",
    "            main_img = cv2.imread(imfile1)\n",
    "            faces = cascade.detectMultiScale(main_img, minNeighbors=12)\n",
    "            for f in faces:\n",
    "                x, y, w, h = [ v for v in f ]\n",
    "                cv2.rectangle(main_img, (x,y), (x+w,y+h), (255,255,255))\n",
    "                sub_face = main_img[y:y+h, x:x+w]\n",
    "                face_file_name = \"dataset\\\\\"+ file +\".jpg\"\n",
    "                \n",
    "                sub_face = cv2.resize(sub_face, (128,128),interpolation=cv2.INTER_AREA)\n",
    "                img = cv2.cvtColor(sub_face,cv2.COLOR_BGR2RGB)\n",
    "                gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "                \n",
    "                vertical_img = cv2.flip(gray, 1)\n",
    "                \n",
    "                X.append(gray)\n",
    "                X.append(vertical_img)\n",
    "                Y.append(class_val(file[0]))\n",
    "                Y.append(class_val(file[0]))\n",
    "#                 class_cnn(file[0])\n",
    "#                 Gender.append(class_val(file[1]))\n",
    "#                 Gender.append(class_val(file[1]))\n",
    "#                 face_flip = \"dataset\\\\\"+file+\"1.jpg\"\n",
    "#                 X.append()\n",
    "#                 cv2.imwrite(face_file_name, gray)\n",
    "#                 cv2.imwrite(face_flip,vertical_img)\n",
    "\n",
    "#             \n",
    "#             vertical_img = cv2.flip(main_img, 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_probability = 0.7\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "num_classes = 4\n",
    "learning_rate = 0.00125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_cnn = X\n",
    "class_cnn = Y\n",
    "# bb = Gender\n",
    "data_cnn1 = np.zeros((len(data_cnn),64,64))\n",
    "for i in range(len(data_cnn)):\n",
    "    data_cnn1[i] = cv2.resize(data_cnn[i], (64,64),interpolation=cv2.INTER_AREA)\n",
    "data_cnn1.shape\n",
    "# from sklearn.utils import shuffle\n",
    "# data_cnn, class_cnn = shuffle(data_cnn,class_cnn, random_state=123)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(data_cnn1, class_cnn, test_size=0.2, random_state=123)\n",
    "X_train_cnn = X_train_cnn.reshape(1976,64,64,1)\n",
    "X_test_cnn = X_test_cnn.reshape(494,64,64,1)\n",
    "# len(X_test_cnn)\n",
    "y_train_cnn = keras.utils.to_categorical(y_train_cnn, num_classes)\n",
    "y_test_cnn = keras.utils.to_categorical(y_test_cnn, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# x = tf.placeholder(tf.float32, shape=(None, 64,64,1), name='input_x')\n",
    "# y =  tf.placeholder(tf.float32, shape=(None, 4), name='output_y')\n",
    "# keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(64,64,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation(tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1976 samples, validate on 494 samples\n",
      "Epoch 1/10\n",
      "1976/1976 [==============================] - 97s 49ms/step - loss: 9.2153 - acc: 0.4261 - val_loss: 9.3315 - val_acc: 0.4211\n",
      "Epoch 2/10\n",
      "1976/1976 [==============================] - 96s 49ms/step - loss: 9.1031 - acc: 0.4352 - val_loss: 9.3315 - val_acc: 0.4211\n",
      "Epoch 3/10\n",
      "1976/1976 [==============================] - 90s 46ms/step - loss: 9.1031 - acc: 0.4352 - val_loss: 9.3315 - val_acc: 0.4211\n",
      "Epoch 4/10\n",
      "1976/1976 [==============================] - 95s 48ms/step - loss: 9.1031 - acc: 0.4352 - val_loss: 9.3315 - val_acc: 0.4211\n",
      "Epoch 5/10\n",
      "1976/1976 [==============================] - 96s 49ms/step - loss: 9.1031 - acc: 0.4352 - val_loss: 9.3315 - val_acc: 0.4211\n",
      "Epoch 6/10\n",
      "1976/1976 [==============================] - 96s 49ms/step - loss: 9.1031 - acc: 0.4352 - val_loss: 9.3315 - val_acc: 0.4211\n",
      "Epoch 7/10\n",
      "1976/1976 [==============================] - 95s 48ms/step - loss: 9.1031 - acc: 0.4352 - val_loss: 9.3315 - val_acc: 0.4211\n",
      "Epoch 8/10\n",
      "1976/1976 [==============================] - 94s 48ms/step - loss: 9.1031 - acc: 0.4352 - val_loss: 9.3315 - val_acc: 0.4211\n",
      "Epoch 9/10\n",
      "1976/1976 [==============================] - 94s 47ms/step - loss: 9.1031 - acc: 0.4352 - val_loss: 9.3315 - val_acc: 0.4211\n",
      "Epoch 10/10\n",
      "1976/1976 [==============================] - 97s 49ms/step - loss: 9.1031 - acc: 0.4352 - val_loss: 9.3315 - val_acc: 0.4211\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test_cnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b3dd9ca77900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           validation_data=(X_test_cnn, y_test_cnn))\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test_cnn' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_cnn, y_train_cnn,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_cnn, y_test_cnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 9.331529130819838\n",
      "Test accuracy: 0.4210526318202617\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_cnn, y_test_cnn, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.zeros((len(X),16385))\n",
    "for i in range(len(X)):\n",
    "    data[i,:-1] = np.reshape(X[i],16384)\n",
    "    data[i, -1] = Y[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "random.shuffle(data)\n",
    "split = int(0.85*data.shape[0])\n",
    "x_train = data[:split,:-1]\n",
    "x_test = data[split:,:-1]\n",
    "y_train = data[:split,-1]\n",
    "y_test = data[split:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred_SVM = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99        43\n",
      "         1.0       0.46      1.00      0.63       144\n",
      "         2.0       1.00      0.16      0.28        43\n",
      "         3.0       1.00      0.08      0.14       141\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       371\n",
      "   macro avg       0.87      0.55      0.51       371\n",
      "weighted avg       0.79      0.55      0.45       371\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(y_test, y_pred_SVM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 42   1   0   0]\n",
      " [  0 144   0   0]\n",
      " [  0  36   7   0]\n",
      " [  0 130   0  11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, y_pred_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5498652291105122"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Neighbor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        test = X_test.shape[0]\n",
    "        y_pred = np.zeros(test, dtype = self.y_train.dtype)\n",
    "        for i in range(test):\n",
    "            distance = np.sum(np.abs(self.X_train - X_test[i,:]), axis = 1)\n",
    "            ind = np.argmin(distance)\n",
    "            y_pred[i] = self.y_train[ind]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7735849056603774"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = Neighbor()\n",
    "nn.train(x_train, y_train)\n",
    "y_pred = nn.predict(x_test)\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      1.00      0.61        43\n",
      "         1.0       0.94      1.00      0.97       144\n",
      "         2.0       0.60      0.63      0.61        43\n",
      "         3.0       0.97      0.52      0.68       141\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       371\n",
      "   macro avg       0.74      0.79      0.72       371\n",
      "weighted avg       0.85      0.77      0.77       371\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 43   0   0   0]\n",
      " [  0 144   0   0]\n",
      " [  9   5  27   2]\n",
      " [ 45   5  18  73]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def cnn(x, keep_prob):\n",
    "#     conv1_filter = tf.Variable(tf.truncated_normal(shape=[3,3, 1, 64], mean=0, stddev=0.08))\n",
    "#     conv2_filter = tf.Variable(tf.truncated_normal(shape=[3,3, 64, 128], mean=0, stddev=0.08))\n",
    "    \n",
    "#     conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "#     conv1 = tf.nn.relu(conv1)\n",
    "#     conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "#     conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "    \n",
    "#     conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "#     conv2 = tf.nn.relu(conv2)\n",
    "#     conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "#     conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "    \n",
    "#     flat = tf.contrib.layers.flatten(conv2_bn)  \n",
    "    \n",
    "#     full = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=1024, activation_fn=tf.nn.relu)\n",
    "#     full = tf.nn.dropout(full, keep_prob)\n",
    "#     full = tf.layers.batch_normalization(full)        \n",
    "    \n",
    "#     out = tf.contrib.layers.fully_connected(inputs=full, num_outputs=4, activation_fn=None)\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logits = cnn(x, keep_prob)\n",
    "\n",
    "# # Loss and Optimizer\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# # Accuracy\n",
    "# correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "# def train_neural_network(session, optimizer, keep_probability, X, y):\n",
    "#     session.run(optimizer, \n",
    "#                 feed_dict={\n",
    "#                     x: X,\n",
    "#                     y: y,\n",
    "#                     keep_prob: keep_probability\n",
    "#                 })\n",
    "    \n",
    "# with tf.Session() as sess:\n",
    "#     # Initializing the variables\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     # Training cycle\n",
    "#     train_neural_network(sess, optimizer, keep_probability, X_train_cnn, y_train_cnn)\n",
    "#     print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_count=100\n",
    "# reg_val=[]\n",
    "# lr_val=[]\n",
    "# test_loss=[]\n",
    "# test_acc=[]\n",
    "\n",
    "# for i in range(max_count):\n",
    "#     reg=10**(np.random.uniform(-4,0) )\n",
    "#     lr=10**(np.random.uniform(-3,-4) )\n",
    "\n",
    "# #Defining the architechture\n",
    "\n",
    "# model=models.Sequential()\n",
    "\n",
    "# model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(60,60,3)))\n",
    "\n",
    "# model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "\n",
    "# model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "# model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "\n",
    "# model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "\n",
    "# model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "\n",
    "# model.add(layers.Dense(512,activation='relu',kernel_regularizer=regularizers.l2(reg)))\n",
    "\n",
    "# model.add(layers.Dense(1,activation='sigmoid',kernel_regularizer=regularizers.l2(reg)))\n",
    "\n",
    "# train_generator=train_datagen.flow_from_directory(\n",
    "#                 train_dir,\n",
    "#                 target_size=(60,60),\n",
    "#                 batch_size=20,\n",
    "#                 class_mode='binary')\n",
    "# test_generator=test_datagen.flow_from_directory(\n",
    "#                 test_dir,\n",
    "#                 target_size=(60,60),\n",
    "#                 batch_size=20,\n",
    "#                 class_mode='binary'\n",
    "#                 )\n",
    "# history=model.fit_generator(train_generator,steps_per_epoch=100,epochs=5,validation_data=test_generator,validation_steps=50)\n",
    "\n",
    "# reg_val.append(reg)\n",
    "# lr_val.append(lr)\n",
    "# test_loss.append(history.history['val_loss'])\n",
    "# test_acc.append(history.history['val_acc'])\n",
    "\n",
    "# print (\"*\"*30)\n",
    "# print (\"Finding the highest Test Accuracy and lowest Test Loss...\")\n",
    "\n",
    "# index1=0\n",
    "# index2=0\n",
    "# max_test_acc=max(test_acc[0])\n",
    "# min_test_loss=min(test_loss[0])\n",
    "# for i in range(max_count):\n",
    "# temp1=max(test_acc[i])\n",
    "# if(temp1>=max_test_acc):\n",
    "#     max_test_acc=temp1\n",
    "#     index1=i\n",
    "#     temp2=min(test_loss[i])\n",
    "# if(temp2<min_test_loss):\n",
    "#     min_test_loss=temp2\n",
    "#     index2=i \n",
    "\n",
    "# print ('Maximum Testing Accuracy:',max_test_acc)\n",
    "# print ('Minimum Testing Loss:',min_test_loss)\n",
    "# print ('Value of optimum learning rate :',lr_val[index1])\n",
    "# print ('Value of optimum regularization:',reg_val[index2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ds1 = os.listdir(\"dataset\")\n",
    "# filenames =[]\n",
    "# for file in ds1:\n",
    "#     if file.endswith(\".jpg\"):\n",
    "#         filenames.append(file)\n",
    "# filenames.sort()\n",
    "# random.seed(123)\n",
    "# random.shuffle(filenames)\n",
    "# split_1 = int(0.8 * len(filenames))\n",
    "# split_2 = int(0.9 * len(filenames))\n",
    "# train_filenames = filenames[:split_1]\n",
    "# dev_filenames = filenames[split_1:split_2]\n",
    "# test_filenames = filenames[split_2:]\n",
    "# #     print (len(filenames))\n",
    "# y_train = []\n",
    "# y_dev = []\n",
    "# y_test = []\n",
    "# for i in train_filenames:\n",
    "#     y_train.append(class_val(i[0]))\n",
    "# for j in dev_filenames:\n",
    "#     y_dev.append(class_val(j[0]))\n",
    "# for k in test_filenames:\n",
    "#     y_test.append(class_val(k[0]))\n",
    "    \n",
    "# train_filenames = np.array(train_filenames)\n",
    "# test_filenames =np.array(test_filenames)\n",
    "# dev_filenames = np.array(dev_filenames)\n",
    "# y_train = np.array(y_train)\n",
    "# y_dev = np.array(y_dev)\n",
    "# y_test = np.array(y_test)\n",
    "\n",
    "# # l = [\"A\" = , \"B\", \"L\", \"W\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# img = cv2.imread(\"dataset\\CFD 2.0.3 Images\\WM-258\\CFD-WM-258-125-N.jpg\")\n",
    "# facedata = \"haarcascade_frontalface_default.xml\"\n",
    "# cascade = cv2.CascadeClassifier(facedata)\n",
    "# faces = cascade.detectMultiScale(img)\n",
    "# for f in faces:\n",
    "#     x, y, w, h = [ v for v in f ]\n",
    "#     cv2.rectangle(img, (x,y), (x+w,y+h), (255,255,255))\n",
    "\n",
    "#     sub_face = img[y:y+h, x:x+w]\n",
    "#     face_file_name = \"dataset\\CFD 2.0.3 Images\\WM-258\\CFD-WM-258-125-N\" + str(y) + \".jpg\"\n",
    "#     cv2.imwrite(face_file_name, sub_face)\n",
    "# plt.imshow(img)\n",
    "\n",
    " \n",
    "# # cv2.resize(image, (128,128),interpolation=cv2.INTER_AREA)\n",
    "# # vertical_img = cv2.flip(image, 1 )\n",
    "# # img = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "# # gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "# # plt.imshow(vertical_img)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
